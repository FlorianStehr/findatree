{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: [conifers, broadleaf] for all weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:03:21.737028Z",
     "start_time": "2022-10-31T10:03:11.626112Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "import sklearn.ensemble \n",
    "import sklearn.model_selection\n",
    "import sklearn.inspection\n",
    "\n",
    "import findatree.io as io\n",
    "import findatree.descriptions as descriptions\n",
    "\n",
    "# Dictionaries: species_name to ba and vice versa\n",
    "species_id_to_name = descriptions.species_id_to_name()\n",
    "species_name_to_id = descriptions.species_name_to_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:53:34.963608Z",
     "start_time": "2022-10-31T10:53:34.932327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Directory: Processed tnr%.hdf5s\n",
    "dir_hdf5 = r\"C:\\Data\\lwf\\processed\\2021\\hdf5\"\n",
    "\n",
    "# Path to flight-log\n",
    "path_log = r\"C:\\Data\\lwf\\Flugbuch_WZE-2021_digitalisiert.csv\"\n",
    "\n",
    "# Directory: sklearn classifier saving\n",
    "dir_sklearn = r\"C:\\Data\\lwf\\analysis\\221029_random_forest\\sklearn\\v01\\2021\"\n",
    "\n",
    "# Save names:\n",
    "save_name_params = 'params.yaml'\n",
    "save_name_gridcv = 'grid.joblib'\n",
    "save_name_dataset = 'dataset.joblib'\n",
    "# save_name_permutation_test_score = 'permutation_test_score.joblib'\n",
    "# save_name_permutation_feature_importance = 'permutation_feature_importance.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load  features and logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:53:44.979889Z",
     "start_time": "2022-10-31T10:53:41.840362Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(io)\n",
    "\n",
    "# Load features\n",
    "df_original, params_df = io.allhdf5s_crowns_features_to_dataframe(dir_hdf5, crowns_type='crowns_human')\n",
    "\n",
    "# Load logbook\n",
    "log = pd.read_csv(path_log, sep=';', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:53:45.712867Z",
     "start_time": "2022-10-31T10:53:45.650367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tnrs = 46 -> sunny\n",
      "#tnrs = 75 -> cloudy\n",
      "#tnrs = 21 -> mixed\n",
      "\n",
      "#crowns = 1415 -> sunny\n",
      "#crowns = 1942 -> cloudy\n",
      "#crowns = 651 -> mixed\n"
     ]
    }
   ],
   "source": [
    "tnrs = log.Traktnummer.values\n",
    "weathers = log['Wetter-Code'].values\n",
    "tnr_to_weather = dict([(tnr, weather) for tnr, weather in zip(tnrs, weathers)])\n",
    "\n",
    "df_original['weather'] = [tnr_to_weather[tnr] for tnr in df_original.tnr]\n",
    "\n",
    "print(f\"#tnrs = {len(np.unique(df_original[df_original.weather == 0].tnr))} -> sunny\")\n",
    "print(f\"#tnrs = {len(np.unique(df_original[df_original.weather == 1].tnr))} -> cloudy\")\n",
    "print(f\"#tnrs = {len(np.unique(df_original[df_original.weather == 2].tnr))} -> mixed\")\n",
    "print()\n",
    "print(f\"#crowns = {np.sum(df_original.weather == 0)} -> sunny\")\n",
    "print(f\"#crowns = {np.sum(df_original.weather == 1)} -> cloudy\")\n",
    "print(f\"#crowns = {np.sum(df_original.weather == 2)} -> mixed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Clean-up of features\n",
    "* dtype and NaNs\n",
    "* Value related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:53:49.150340Z",
     "start_time": "2022-10-31T10:53:48.934222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#crowns = 4008, original\n",
      "#crowns = 3967, after removal (NaNs, i.e. completely shadowed or dead)\n",
      "#crowns = 3908, after query (bk in [0, 1, 210, 320, 330, 340])\n",
      "#crowns = 3907, after query (kkl <= 3)\n",
      "#crowns = 3876, after query (area_bright / 0.2**2 > 10)\n",
      "#crowns = 3876, after query (perc5_ndre > -1e-1)\n"
     ]
    }
   ],
   "source": [
    "# Copy original\n",
    "df = df_original.copy()\n",
    "print(f\"{r'#crowns'} = {len(df)}, original\")\n",
    "\n",
    "#### dtype and NaNs\n",
    "# Convert bhd_2020 column to float32 dtype\n",
    "df.loc[:, 'bhd_2020'] = pd.to_numeric(df.bhd_2020, errors='coerce')\n",
    "df.bhd_2020 = df.bhd_2020.astype(np.float32)\n",
    "\n",
    "# Convert bk column to int32 dtype\n",
    "df.loc[:, 'bk'] = pd.to_numeric(df.bk, errors='coerce')\n",
    "df.bk = df.bk.astype(np.int32)\n",
    "\n",
    "# Drop NaN containing rows\n",
    "df = df.dropna(axis=0, how='any')   \n",
    "print(f\"{r'#crowns'} = {len(df)}, after removal (NaNs, i.e. completely shadowed or dead)\")\n",
    "\n",
    "\n",
    "#### Value related\n",
    "queries = []\n",
    "\n",
    "# Drop bk > 1\n",
    "query_str = 'bk in [0, 1, 210, 320, 330, 340]'\n",
    "queries.append(query_str)\n",
    "df = df.query(query_str)\n",
    "print(f\"{r'#crowns'} = {len(df)}, after query ({query_str})\")\n",
    "\n",
    "# Drop kkl > 3\n",
    "query_str = 'kkl <= 3'\n",
    "queries.append(query_str)\n",
    "df = df.query(query_str)\n",
    "print(f\"{r'#crowns'} = {len(df)}, after query ({query_str})\")\n",
    "\n",
    "# Drop area_bright/0.2**2 < 10\n",
    "query_str = 'area_bright / 0.2**2 > 10'\n",
    "queries.append(query_str)\n",
    "df = df.query(query_str)\n",
    "print(f\"{r'#crowns'} = {len(df)}, after query ({query_str})\")\n",
    "\n",
    "# Drop perc5_ndre < 0\n",
    "query_str = 'perc5_ndre > -1e-1'\n",
    "queries.append(query_str)\n",
    "df = df.query(query_str)\n",
    "print(f\"{r'#crowns'} = {len(df)}, after query ({query_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign classes: Conifers and Broadleaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:53:53.027107Z",
     "start_time": "2022-10-31T10:53:52.980211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label          |label_name     |label_count\n",
      "--------------------------------------------------\n",
      "0              |conifers       |2921\n",
      "1              |broadleaf      |916\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(descriptions)\n",
    "\n",
    "# Define families by patterns\n",
    "family_patterns = [\n",
    "    'kiefer|fichte|tanne|douglasie|lärche', \n",
    "    'buche|eiche|ahorn|erle|birke|esche',\n",
    "]\n",
    "\n",
    "family_names = [\n",
    "    'conifers',\n",
    "    'broadleaf',\n",
    "]\n",
    "\n",
    "families = descriptions.species_groupby_families(family_patterns, family_names)\n",
    "family_ids = descriptions.species_id_to_family_id(df.ba.values, families)\n",
    "\n",
    "# Assign family_id as class under -> class_id\n",
    "df = df.assign(label = family_ids)\n",
    "\n",
    "# Create a comprehensive dict for labels definition to save in params\n",
    "labels = dict([(family_id, family['family_name']) for (family_id, family) in families.items()])\n",
    "\n",
    "# Print a summary of the classes\n",
    "print(f\"{'label':<15}|{'label_name':<15}|{'label_count'}\")\n",
    "print('-'*50)\n",
    "for label, label_name in labels.items():\n",
    "    print(f\"{label:<15}|{label_name:<15}|{np.sum(df.label == label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search: Pattern in column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '^x_|^y'\n",
    "\n",
    "cols = list(df.columns)\n",
    "for col in cols:\n",
    "    if bool(re.search(pattern, col, re.IGNORECASE)):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:53:56.988281Z",
     "start_time": "2022-10-31T10:53:56.950543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of crowns        : 3876\n",
      "Mean number of crowns per tnr : 27.3\n",
      "__________________________________________________\n",
      "\n",
      "species_id| species_name                  | count\n",
      "--------------------------------------------------\n",
      "       134| Gemeine Kiefer                : 1391\n",
      "       118| Gemeine Fichte                : 1185\n",
      "        20| Rotbuche                      : 443\n",
      "       100| Weißtanne                     : 179\n",
      "        48| Traubeneiche                  : 174\n",
      "        51| Stieleiche                    : 129\n",
      "       116| Europäische Lärche            : 121\n",
      "        10| Gemeine Birke                 : 49\n",
      "        22| Gemeine Esche                 : 44\n",
      "       136| Douglasie                     : 35\n",
      "         7| Schwarzerle                   : 28\n",
      "         5| Bergahorn                     : 18\n",
      "        13| Hainbuche                     : 17\n",
      "        36| Kirsche                       : 14\n",
      "        35| Aspe                          : 9\n",
      "        53| Roteiche                      : 9\n",
      "       129| Schwarzkiefer                 : 9\n",
      "        62| Weide                         : 5\n",
      "       133| Strobe                        : 4\n",
      "         1| Feldahorn                     : 4\n",
      "        68| Winterlinde                   : 3\n",
      "        70| Bergulme                      : 2\n",
      "        56| Robinie                       : 1\n",
      "        65| Speierling                    : 1\n",
      "       103| Küstentanne                   : 1\n",
      "         4| Spitzahorn                    : 1\n",
      "__________________________________________________\n",
      "\n",
      " family_id| family_name                   | count | species_names\n",
      "--------------------------------------------------\n",
      "         0| conifers                      : 2921  | ['Gemeine Fichte', 'Omorikafichte', 'Sitkafichte', 'Gemeine Kiefer', 'Schwarzkiefer', 'Zirbelkiefer', 'Bergkiefer', 'Weißtanne', 'Küstentanne', 'Edeltanne', 'Nordmannstanne', 'Europäische Lärche', 'Japanische Lärche', 'Douglasie']\n",
      "         1| broadleaf                     : 916   | ['Trauben- oder Stieleiche', 'Traubeneiche', 'Stieleiche', 'Zerreiche', 'Roteiche', 'Gemeine Birke', 'Moorbirke', 'Schwarzerle', 'Rotbuche', 'Grauerle', 'Hainbuche', 'Grünerle', 'Gemeine Esche', 'Bergahorn', 'Spitzahorn', 'Feldahorn']\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(descriptions)\n",
    "\n",
    "descriptions.print_summary(\n",
    "    df.tnr.values,\n",
    "    df.ba.values,\n",
    "    df.label.values,\n",
    "    families,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:54:01.658136Z",
     "start_time": "2022-10-31T10:54:01.626922Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'labels':labels,\n",
    "    'test_size': 0.25,\n",
    "    'cv_splits': 5,\n",
    "    'scoring': 'balanced_accuracy',\n",
    "    'n_permutations': 20,\n",
    "    'n_repeats': 10,\n",
    "    'max_samples': 0.5,\n",
    "}\n",
    "\n",
    "# Save parameters\n",
    "io.list_of_dicts_to_yaml(os.path.join(dir_sklearn, save_name_params), [params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define: Features and labels\n",
    "* Define features columns: Exclude terr., identifiers and coords. columns\n",
    "* Define samples: Exclude unassigend samples, based on labels\n",
    "* Define features, labels and extended labels\n",
    "\n",
    "Create extended labels `y_extend` with information about `['family', 'ba', 'sst', 'nbv', 'weather']`, to check later if classfication is dependent on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:54:13.801258Z",
     "start_time": "2022-10-31T10:54:13.732255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (3837, 107)\n",
      "y.shape: (3837,)\n",
      "y_extend.shape: (3837, 7)\n",
      "Label ratios in y:\n",
      "    label[0]/label[0] = 1.00\n",
      "    label[1]/label[0] = 0.31\n"
     ]
    }
   ],
   "source": [
    "#### Define feautures columns\n",
    "\n",
    "# Get all dataset columns\n",
    "x_names = list(df.columns)\n",
    "\n",
    "# Terrestrial feature names to be excluded\n",
    "terr_names = list(params_df[list(params_df.keys())[0]]['features_terrestrial_names'])\n",
    "\n",
    "# Identifiers feature names to be excluded\n",
    "ident_names = [ 'label', 'tnr', 'id', 'weather']\n",
    "\n",
    "# Coordinate features pattern to be excluded\n",
    "coordinate_pattern = '^x_|^y_'\n",
    "\n",
    "# Exclude all\n",
    "x_names = [col for col in x_names if col not in terr_names]\n",
    "x_names = [col for col in x_names if col not in ident_names]\n",
    "x_names = [col for col in x_names if not bool(re.search(coordinate_pattern, col))]\n",
    "\n",
    "\n",
    "#### Define samples: Exclude unassigend samples, based on labels\n",
    "samples_include = df.label >= 0\n",
    "\n",
    "\n",
    "#### Define features, labels and extended classes\n",
    "# Define features -> x\n",
    "x = df.loc[samples_include, x_names].values\n",
    "\n",
    "# Define labels -> y\n",
    "y_names = ['label']\n",
    "y = df.loc[samples_include, y_names[0]].values\n",
    "\n",
    "# Define extended labels -> y_extend\n",
    "y_extend_names = ['label', 'tnr', 'id', 'ba', 'sst', 'nbv', 'weather']\n",
    "y_extend = df.loc[samples_include, y_extend_names].values\n",
    "\n",
    "\n",
    "#### Print infos about  features, labels and extended labels\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"y_extend.shape: {y_extend.shape}\")\n",
    "print(f\"Label ratios in y:\")\n",
    "for i in labels.keys():\n",
    "    print(f\"{' '*4}label[{i}]/label[0] = {np.sum(y == i) / np.sum(y == 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T10:54:20.418433Z",
     "start_time": "2022-10-31T10:54:20.365043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Data\\\\lwf\\\\analysis\\\\221029_random_forest\\\\sklearn\\\\v01\\\\2021\\\\dataset.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, y_extend_train, y_extend_test, = sklearn.model_selection.train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    y_extend,\n",
    "    test_size=params['test_size'],\n",
    "    shuffle=True,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "dataset = {\n",
    "    'x_names': x_names,\n",
    "    'y_names': y_names,\n",
    "    'y_extend_names': y_extend_names,\n",
    "    'x_train': x_train,\n",
    "    'x_test': x_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'y_extend_train': y_extend_train,\n",
    "    'y_extend_test': y_extend_test,\n",
    "}\n",
    "\n",
    "# Save dataset\n",
    "joblib.dump(dataset, os.path.join(dir_sklearn, save_name_dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T11:00:21.625765Z",
     "start_time": "2022-10-31T10:56:09.084444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'criterion': 'entropy', 'max_samples': 0.75, 'min_samples_leaf': 1, 'n_estimators': 200}\n",
      "Scoring: balanced_accuracy\n",
      "Test score: 0.97 (accuracy)\n",
      "Test score: 0.94 (balanced_accuracy)\n"
     ]
    }
   ],
   "source": [
    "params_grd = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'max_samples': [0.75, 0.5]\n",
    "}\n",
    "grd = sklearn.model_selection.GridSearchCV(\n",
    "    sklearn.ensemble.RandomForestClassifier(n_jobs=-1),\n",
    "    param_grid = params_grd,\n",
    "    scoring  = params['scoring'],\n",
    "    cv = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=params['cv_splits'],\n",
    "        test_size=params['test_size'],\n",
    "    )\n",
    ")\n",
    "\n",
    "grd.fit(x_train, y_train)\n",
    "\n",
    "# Save grid\n",
    "joblib.dump(grd, os.path.join(dir_sklearn, save_name_gridcv))\n",
    "\n",
    "#### Info about grid search\n",
    "\n",
    "# Get best estimator prediction for x_test\n",
    "y_test_pred = grd.best_estimator_.predict(x_test)\n",
    "\n",
    "# Best estimator\n",
    "print(f\"Params: {grd.best_params_}\")\n",
    "print(f\"Scoring: {grd.scoring}\")\n",
    "print(f\"Test score: {sklearn.metrics.accuracy_score(y_test, y_test_pred):.2f} (accuracy)\")\n",
    "print(f\"Test score: {sklearn.metrics.balanced_accuracy_score(y_test, y_test_pred):.2f} (balanced_accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Estimator: Permutation Test Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# score, permutation_score, pvalue = sklearn.model_selection.permutation_test_score(\n",
    "#     grd.best_estimator_,\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "#     cv=sklearn.model_selection.KFold(n_splits=params['cv_splits'], shuffle=params['shuffle']),\n",
    "#     n_permutations=params['n_permutations'],\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "\n",
    "# # Add result to params\n",
    "# permutation_test_score = {\n",
    "#     'permutation_scores':  permutation_score,\n",
    "#     'test_score': score,\n",
    "#     'pvalue': pvalue,\n",
    "# }\n",
    "\n",
    "# # Save permutation_test_score\n",
    "# joblib.dump(permutation_test_score, os.path.join(dir_sklearn, save_name_permutation_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Estimator: Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Data\\\\lwf\\\\analysis\\\\220830_random-forrest\\\\sklearn\\\\v01\\\\permutation_feature_importance.joblib']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perm_imp_train = sklearn.inspection.permutation_importance(\n",
    "#     grd.best_estimator_,\n",
    "#     x_test,\n",
    "#     y_test,\n",
    "#     n_repeats=params['n_repeats'],\n",
    "#     max_samples=params['max_samples'],\n",
    "# )\n",
    "\n",
    "# # Add result to params\n",
    "# permutation_feature_importance = {\n",
    "#     'on': 'test',\n",
    "#     'importances_mean': perm_imp_train['importances_mean'],\n",
    "#     'importances_std': perm_imp_train['importances_std'],\n",
    "#     'importances': perm_imp_train['importances'],\n",
    "# }\n",
    "\n",
    "# # Save permutation feature importances\n",
    "# joblib.dump(permutation_feature_importance, os.path.join(dir_sklearn, save_name_permutation_feature_importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load: Previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_name = save_name_gridcv\n",
    "# grd = joblib.load(os.path.join(dir_sklearn, load_name))\n",
    "\n",
    "# load_name = save_name_params\n",
    "# with open(os.path.join(dir_sklearn, load_name), \"r\") as f:\n",
    "#     params = yaml.safe_load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "635c494a99b4919ffc46b3179c211e3df0819f0dc50ebdacd534597eabf9f7f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
